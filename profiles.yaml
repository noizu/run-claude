# run-claude profiles
#
# Profile search order (first match wins):
# 1. ~/.config/run-claude/user.profiles.yaml
# 2. ~/.config/run-claude/profiles.yaml
# 3. <install>/user.profiles.yaml
# 4. <install>/profiles.yaml
#
# To disable a profile from a higher-priority file, set model: null or model: false
# Example:
#   cerebras:
#     model: null  # This disables cerebras, falling through to lower-priority files

# Anthropic - Native Claude models
anthropic:
  name: "Anthropic"
  opus_model: "claude-opus-4-20250514"
  sonnet_model: "claude-sonnet-4-20250514"
  haiku_model: "claude-3-5-haiku-20241022"

# OpenAI - GPT models
openai:
  name: "OpenAI"
  opus_model: "openai-gpt4"
  sonnet_model: "openai-gpt4o"
  haiku_model: "openai-gpt4o-mini"

# Cerebras - Speed optimized inference
cerebras:
  name: "Cerebras"
  opus_model: "cerebras/gpt-oss-120b"
  sonnet_model: "cerebras/qwen-3-32b"
  haiku_model: "cerebras/llama3.1-8b"

# Cerebras2 - Alternative Cerebras config with GLM
cerebras2:
  name: "Cerebras (GLM)"
  opus_model: "cerebras/zai-glm-4.6"
  sonnet_model: "cerebras/gpt-oss-120b"
  haiku_model: "cerebras/qwen-3-32b"

# Cerebras Subscription - GLM with thinking tiers
cerebras-pro:
  name: "Cerebras Pro Subscription"
  opus_model: "cerebras-pro/opus"
  sonnet_model: "cerebras-pro/sonnet"
  haiku_model: "cerebras-pro/haiku"

groq-pro:
  name: "Groq Equivalent"
  opus_model: "groq/opus"
  sonnet_model: "groq/sonnet"
  haiku_model: "groq/haiku"
  extended:
    - "groq-kimi-k2"
    - "groq-llama-4-scout"
    - "groq-llama-4-maverick"
    - "groq-qwen3-32b"
    - "groq-llama-3.3-70b"
    - "groq-llama-3.1-8b"


# Z.AI Subscription - GLM with thinking tiers
zai-pro:
  name: "Zai Subscription"
  opus_model: "zai/opus"
  sonnet_model: "zai/sonnet"
  haiku_model: "zai/haiku"


# glm 4.7 group
fast-glm:
  name: "Cerebras Pro Subscription"
  opus_model: "zai/opus"
  sonnet_model: "groq/opus"
  haiku_model: "cerebras-pro/opus"

# Groq - Ultra-fast inference
groq:
  name: "Groq"
  opus_model: "groq.qwen3-32b"
  sonnet_model: "groq.kimi-k2-instruct"
  haiku_model: "groq.gpt-oss-120b"

# Groq2 - Alternative Groq config (speed optimized)
groq2:
  name: "Groq (Speed)"
  opus_model: "groq.gpt-oss-120b"
  sonnet_model: "groq.qwen3-32b"
  haiku_model: "groq.llama-3.3-70b"

# Groq Mix - Optimized Groq blend (intelligence-first)
groq-mix:
  name: "Groq Mix (Intelligence-First)"
  opus_model: "groq.gpt-oss-120b"
  sonnet_model: "groq.kimi-k2-instruct"
  haiku_model: "groq.qwen3-32b"
  extended:
    - "groq.llama-3.3-70b"
    - "groq.llama-3.1-8b"
    - "groq.llama-4-scout"
    - "groq.llama-4-maverick"

# Gemini - Google AI models
gemini:
  name: "Google Gemini"
  opus_model: "gemini-pro"
  sonnet_model: "gemini-flash"
  haiku_model: "gemini-flash-lite"

# Azure OpenAI - Enterprise Azure deployment
azure:
  name: "Azure OpenAI"
  opus_model: "azure-gpt4"
  sonnet_model: "azure-gpt4o"
  haiku_model: "azure-gpt4o-mini"

# Grok - xAI models
grok:
  name: "xAI Grok"
  opus_model: "grok-2"
  sonnet_model: "grok-2"
  haiku_model: "grok-2-mini"

# DeepSeek - DeepSeek AI models
deepseek:
  name: "DeepSeek"
  opus_model: "deepseek-reasoner"
  sonnet_model: "deepseek-chat"
  haiku_model: "deepseek-coder"

# Mistral - Mistral AI models
mistral:
  name: "Mistral AI"
  opus_model: "mistral-large"
  sonnet_model: "mistral-medium"
  haiku_model: "mistral-small"

# Perplexity - Search-enabled inference
perplexity:
  name: "Perplexity (Search-Augmented)"
  opus_model: "perplexity-large"
  sonnet_model: "perplexity-medium"
  haiku_model: "perplexity-fast"

# Local - Ollama / vLLM / LM Studio
local:
  name: "Local/Self-Hosted"
  opus_model: "local-large"
  sonnet_model: "local-medium"
  haiku_model: "local-small"

# Multi - Best-of-breed from multiple providers
multi:
  name: "Multi-Provider (Best of Breed)"
  opus_model: "openai-gpt4"
  sonnet_model: "gemini-flash"
  haiku_model: "gemini-flash-lite"
